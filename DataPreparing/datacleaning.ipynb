{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8a9a2e2-69cb-4bdb-a8a3-ed765d411b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    }
   ],
   "source": [
    "!/opt/homebrew/Cellar/jupyterlab/4.4.2_1/libexec/bin/python -m pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "456427e9-0410-446f-94c3-c312516b1b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize Turkish characters and handle case for consistent comparison.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    text = text.lower()\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    mappings = {\n",
    "        's': ['ş', 's'],\n",
    "        'i': ['ı', 'i'],\n",
    "        'g': ['ğ', 'g'],\n",
    "        'u': ['ü', 'u'],\n",
    "        'c': ['ç', 'c'],\n",
    "        'o': ['ö', 'o']\n",
    "    }\n",
    "    for target, sources in mappings.items():\n",
    "        for source in sources:\n",
    "            text = text.replace(source, target)\n",
    "    return text.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7ee9310-1fba-4209-8df6-97605bbd9c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_station_name(row, station_list, line_map):\n",
    "    \"\"\"Clean station name and match exactly to reference list, considering line_name.\"\"\"\n",
    "    station = row['station_poi_desc_cd']\n",
    "    line_name = row['line_name'].upper()\n",
    "    if not isinstance(station, str):\n",
    "        return station\n",
    "    \n",
    "    explicit_mappings = {\n",
    "        'KISIKLI': 'Kısıklı',\n",
    "        'HACIOSMAN': 'Hacıosman',\n",
    "        'KADIKOY': 'Kadıköy',\n",
    "        'KADIKOY ': 'Kadıköy',\n",
    "        ' KADIKOY': 'Kadıköy',\n",
    "        'ACIBADEM': 'Acıbadem',\n",
    "        'IMAM HATIP LISESI': 'İmam H. Lisesi',\n",
    "        'SOGANLIK': 'Soğanlık',\n",
    "        'ITU GUNEY': 'İTÜ-Ayazağa',\n",
    "        'SANAYI MAH. GUNEY': 'Sanayi Mahallesi',\n",
    "        'SISLI GUNEY': 'Şişli-Mecidiyeköy',\n",
    "        'IHLAMUR KUYU': 'Ihlamurkuyu',\n",
    "        'AYRILIKCESME': 'Ayrılık Çeşmesi',\n",
    "        'YAKACIK': 'Yakacık',\n",
    "        'NECİP FAZIL': 'Necip Fazıl',\n",
    "        'SAGMALCILAR': 'Sağmalcılar',\n",
    "        'BAKIRKOY': 'Bakırköy',\n",
    "        'BAGCILAR MEYDAN': 'Bağcılar Meydan',\n",
    "        'SANAYI MAH.': 'Sanayi Mahallesi',\n",
    "        ' SANAYI MAH.': 'Sanayi Mahallesi',\n",
    "        'SANAYI MAH. ': 'Sanayi Mahallesi'\n",
    "    }\n",
    "    \n",
    "    station_upper = station.upper().strip()\n",
    "    for key in explicit_mappings:\n",
    "        if normalize_text(station_upper) == normalize_text(key):\n",
    "            return explicit_mappings[key]\n",
    "    \n",
    "    cleaned = station\n",
    "    cleaned = re.sub(r'\\s*\\([^)]*\\)', '', cleaned, flags=re.IGNORECASE)\n",
    "    suffixes = [\n",
    "        r'\\s*(güney|güney|kuzey|batı|bati|doğu|dogu|giriş|giris|çıkış|cikis|\\d+)\\s*$',\n",
    "        r'\\s*(south|north|west|east|entry|exit|\\d+)\\s*$'\n",
    "    ]\n",
    "    for suffix in suffixes:\n",
    "        cleaned = re.sub(suffix, '', cleaned, flags=re.IGNORECASE)\n",
    "    \n",
    "    normalized_cleaned = normalize_text(cleaned)\n",
    "    \n",
    "    candidate_stations = [s for s, l in station_list if l == line_name] if line_name in line_map else [s for s, _ in station_list]\n",
    "    \n",
    "    for ref_station in candidate_stations:\n",
    "        normalized_ref = normalize_text(ref_station)\n",
    "        if normalized_cleaned == normalized_ref:\n",
    "            return ref_station\n",
    "        if '-' in ref_station:\n",
    "            parts = ref_station.split('-')\n",
    "            for part in parts:\n",
    "                if normalize_text(part) == normalized_cleaned:\n",
    "                    return ref_station\n",
    "        if normalized_cleaned in normalized_ref or normalized_ref in normalized_cleaned:\n",
    "            return ref_station\n",
    "    \n",
    "    for ref_station, _ in station_list:\n",
    "        normalized_ref = normalize_text(ref_station)\n",
    "        if normalized_cleaned == normalized_ref or normalized_cleaned in normalized_ref or normalized_ref in normalized_cleaned:\n",
    "            return ref_station\n",
    "    \n",
    "    return cleaned \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "540e0af0-adf0-48c7-b37d-11fcd50d0087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_transport_data(input_file, stations_file, output_file):\n",
    "    \"\"\"Main function to clean transportation dataset.\"\"\"\n",
    "    \n",
    "    df = pd.read_csv(input_file)\n",
    "    stations_df = pd.read_csv(stations_file)\n",
    "    \n",
    "    station_list = list(zip(stations_df['Station Name'], stations_df['Line Code'].str.upper()))\n",
    "    \n",
    "    line_map = {}\n",
    "    for station, line in station_list:\n",
    "        if line not in line_map:\n",
    "            line_map[line] = []\n",
    "        line_map[line].append(station)\n",
    "    \n",
    "    columns_to_remove = [\n",
    "        'transport_type_id', 'transfer_type', 'number_of_passage', \n",
    "        'product_kind', 'transaction_type_desc', 'town'\n",
    "    ]\n",
    "    df = df.drop(columns=[col for col in columns_to_remove if col in df.columns])\n",
    "    \n",
    "    df = df[df['road_type'].str.upper() == 'RAYLI']\n",
    "    \n",
    "    valid_lines = ['M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'M11']\n",
    "    df = df[df['line_name'].str.upper().isin(valid_lines)]\n",
    "    \n",
    "    df['station_poi_desc_cd'] = df.apply(\n",
    "        lambda row: clean_station_name(row, station_list, line_map), axis=1\n",
    "    )\n",
    "    \n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Cleaned data saved to {output_file}\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90aec734-992c-45d5-bbfe-033243f3f2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to ../Data/output/september2024cleaned_transport_data.csv\n",
      "Data cleaning completed.\n",
      "Number of records after cleaning: 614229\n"
     ]
    }
   ],
   "source": [
    "input_file = \"../Data/september2024.csv\" \n",
    "stations_file = \"../Data/metrostationsbyorder.csv\"  \n",
    "output_file = \"../Data/output/september2024cleaned_transport_data.csv\"  \n",
    "\n",
    "cleaned_df = clean_transport_data(input_file, stations_file, output_file)\n",
    "print(\"Data cleaning completed.\")\n",
    "print(f\"Number of records after cleaning: {len(cleaned_df)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
